{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OCR_model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RY1daF_lQPsl",
        "6KU2dJsnQZZl",
        "ubxNjlGfQksD",
        "oItQzlw_V-xb",
        "XEsodYOWYC5p",
        "r3DfWPL5XYYF",
        "N5I33FChXk9K",
        "gkCUZh0GZgtq",
        "en4FEtwTcIrp"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY1daF_lQPsl"
      },
      "source": [
        "# Baseline Model with Multi-Layer Perceptrons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uop7j7I3DpFQ",
        "outputId": "d9f80f1a-daa2-49a7-c323-bdbcea549727"
      },
      "source": [
        "# Baseline MLP for MNIST dataset\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import np_utils\n",
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "# flatten 28*28 images to a 784 vector for each image\n",
        "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
        "X_train = X_train.reshape((X_train.shape[0], num_pixels)).astype('float32')\n",
        "X_test = X_test.reshape((X_test.shape[0], num_pixels)).astype('float32')\n",
        "# normalize inputs from 0-255 to 0-1\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "# one hot encode outputs\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]\n",
        "# define baseline model\n",
        "def baseline_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
        "\tmodel.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model\n",
        "# build the model\n",
        "model = baseline_model()\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "300/300 - 4s - loss: 0.2782 - accuracy: 0.9208 - val_loss: 0.1389 - val_accuracy: 0.9597\n",
            "Epoch 2/10\n",
            "300/300 - 1s - loss: 0.1084 - accuracy: 0.9688 - val_loss: 0.0899 - val_accuracy: 0.9721\n",
            "Epoch 3/10\n",
            "300/300 - 1s - loss: 0.0697 - accuracy: 0.9801 - val_loss: 0.0737 - val_accuracy: 0.9771\n",
            "Epoch 4/10\n",
            "300/300 - 1s - loss: 0.0508 - accuracy: 0.9853 - val_loss: 0.0698 - val_accuracy: 0.9783\n",
            "Epoch 5/10\n",
            "300/300 - 1s - loss: 0.0365 - accuracy: 0.9895 - val_loss: 0.0665 - val_accuracy: 0.9803\n",
            "Epoch 6/10\n",
            "300/300 - 1s - loss: 0.0260 - accuracy: 0.9932 - val_loss: 0.0675 - val_accuracy: 0.9788\n",
            "Epoch 7/10\n",
            "300/300 - 1s - loss: 0.0197 - accuracy: 0.9950 - val_loss: 0.0646 - val_accuracy: 0.9794\n",
            "Epoch 8/10\n",
            "300/300 - 1s - loss: 0.0141 - accuracy: 0.9968 - val_loss: 0.0583 - val_accuracy: 0.9816\n",
            "Epoch 9/10\n",
            "300/300 - 1s - loss: 0.0109 - accuracy: 0.9976 - val_loss: 0.0562 - val_accuracy: 0.9816\n",
            "Epoch 10/10\n",
            "300/300 - 1s - loss: 0.0084 - accuracy: 0.9982 - val_loss: 0.0636 - val_accuracy: 0.9806\n",
            "Baseline Error: 1.94%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KU2dJsnQZZl"
      },
      "source": [
        "# Simple Convolutional Neural Network for MNIST\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7ofSzJVQcRs",
        "outputId": "66fa2581-6190-4ad6-eee6-a82a19719345"
      },
      "source": [
        "# Simple CNN for the MNIST Dataset\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "# reshape to be [samples][width][height][channels]\n",
        "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32')\n",
        "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float32')\n",
        "# normalize inputs from 0-255 to 0-1\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "# one hot encode outputs\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]\n",
        "# define a simple CNN model\n",
        "def baseline_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
        "\tmodel.add(MaxPooling2D())\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(128, activation='relu'))\n",
        "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model\n",
        "# build the model\n",
        "model = baseline_model()\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "300/300 [==============================] - 30s 8ms/step - loss: 0.2386 - accuracy: 0.9314 - val_loss: 0.0712 - val_accuracy: 0.9777\n",
            "Epoch 2/10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.0732 - accuracy: 0.9781 - val_loss: 0.0498 - val_accuracy: 0.9843\n",
            "Epoch 3/10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.0520 - accuracy: 0.9841 - val_loss: 0.0513 - val_accuracy: 0.9835\n",
            "Epoch 4/10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.0408 - accuracy: 0.9873 - val_loss: 0.0347 - val_accuracy: 0.9887\n",
            "Epoch 5/10\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.0337 - accuracy: 0.9895 - val_loss: 0.0353 - val_accuracy: 0.9882\n",
            "Epoch 6/10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.0288 - accuracy: 0.9909 - val_loss: 0.0318 - val_accuracy: 0.9886\n",
            "Epoch 7/10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.0219 - accuracy: 0.9933 - val_loss: 0.0334 - val_accuracy: 0.9898\n",
            "Epoch 8/10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.0188 - accuracy: 0.9941 - val_loss: 0.0345 - val_accuracy: 0.9892\n",
            "Epoch 9/10\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.0173 - accuracy: 0.9944 - val_loss: 0.0307 - val_accuracy: 0.9900\n",
            "Epoch 10/10\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.0162 - accuracy: 0.9950 - val_loss: 0.0302 - val_accuracy: 0.9903\n",
            "CNN Error: 0.97%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubxNjlGfQksD"
      },
      "source": [
        "# Larger Convolutional Neural Network for MNIST\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzLrZgeNQlVb",
        "outputId": "599850e4-c737-4cf8-f96e-5c759b0bf96a"
      },
      "source": [
        "# Larger CNN for the MNIST Dataset\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "# reshape to be [samples][width][height][channels]\n",
        "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32')\n",
        "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float32')\n",
        "# normalize inputs from 0-255 to 0-1\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "# one hot encode outputs\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]\n",
        "# define the larger model\n",
        "def larger_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(30, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
        "\tmodel.add(MaxPooling2D())\n",
        "\tmodel.add(Conv2D(15, (3, 3), activation='relu'))\n",
        "\tmodel.add(MaxPooling2D())\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(128, activation='relu'))\n",
        "\tmodel.add(Dense(50, activation='relu'))\n",
        "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model\n",
        "# build the model\n",
        "model = larger_model()\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "300/300 [==============================] - 5s 14ms/step - loss: 0.3678 - accuracy: 0.8843 - val_loss: 0.0751 - val_accuracy: 0.9758\n",
            "Epoch 2/10\n",
            "300/300 [==============================] - 4s 12ms/step - loss: 0.0935 - accuracy: 0.9710 - val_loss: 0.0480 - val_accuracy: 0.9847\n",
            "Epoch 3/10\n",
            "300/300 [==============================] - 3s 12ms/step - loss: 0.0700 - accuracy: 0.9786 - val_loss: 0.0391 - val_accuracy: 0.9875\n",
            "Epoch 4/10\n",
            "300/300 [==============================] - 3s 11ms/step - loss: 0.0577 - accuracy: 0.9815 - val_loss: 0.0311 - val_accuracy: 0.9897\n",
            "Epoch 5/10\n",
            "300/300 [==============================] - 3s 11ms/step - loss: 0.0478 - accuracy: 0.9845 - val_loss: 0.0326 - val_accuracy: 0.9895\n",
            "Epoch 6/10\n",
            "300/300 [==============================] - 3s 11ms/step - loss: 0.0429 - accuracy: 0.9867 - val_loss: 0.0279 - val_accuracy: 0.9901\n",
            "Epoch 7/10\n",
            "300/300 [==============================] - 3s 11ms/step - loss: 0.0381 - accuracy: 0.9879 - val_loss: 0.0259 - val_accuracy: 0.9911\n",
            "Epoch 8/10\n",
            "300/300 [==============================] - 3s 11ms/step - loss: 0.0351 - accuracy: 0.9890 - val_loss: 0.0298 - val_accuracy: 0.9906\n",
            "Epoch 9/10\n",
            "300/300 [==============================] - 3s 11ms/step - loss: 0.0325 - accuracy: 0.9900 - val_loss: 0.0259 - val_accuracy: 0.9913\n",
            "Epoch 10/10\n",
            "300/300 [==============================] - 3s 11ms/step - loss: 0.0306 - accuracy: 0.9901 - val_loss: 0.0236 - val_accuracy: 0.9921\n",
            "Large CNN Error: 0.79%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oItQzlw_V-xb"
      },
      "source": [
        "# Mnist +  AZ Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEsodYOWYC5p"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCcParuCX7uC"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "# set the matplotlib backend so figures can be saved in the background\n",
        "import cv2\n",
        "import argparse\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# import the necessary packages\n",
        "from tensorflow.keras.models import load_model\n",
        "from imutils.contours import sort_contours\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import cv2\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# from typing_extensions import Required\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "# import the necessary packages\n",
        "\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import ZeroPadding2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import add\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtbxLNvkWE0b",
        "outputId": "e0e006fb-c6a0-41dd-e8f9-ecb013485271"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!unzip '/content/drive/MyDrive/Programming/Python Projects/OCR Model/A_Z Handwritten Data.csv.zip'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Archive:  /content/drive/MyDrive/Programming/Python Projects/OCR Model/A_Z Handwritten Data.csv.zip\n",
            "replace A_Z Handwritten Data.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3DfWPL5XYYF"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sqUgX6JWB9F"
      },
      "source": [
        "def load_az_dataset(dataset_path):\n",
        "    # initialize the list of data and labels\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    # loop over the rows of the A-Z handwritten digit dataset\n",
        "    for row in open(dataset_path):\n",
        "        # parse the label and image from the row\n",
        "        row = row.split(\",\")\n",
        "        label = int(row[0])\n",
        "        image = np.array([int(x) for x in row[1:]], dtype=\"uint8\")\n",
        "\n",
        "        # images are represented as single channel (grayscale) images\n",
        "        # that are 28x28=784 pixels -- we need to take this flattened\n",
        "        # 784-d list of numbers and reshape them into a 28x28 matrix\n",
        "        image = image.reshape((28, 28))\n",
        "\n",
        "        # update the list of data and labels\n",
        "        data.append(image)\n",
        "        labels.append(label)\n",
        "\n",
        "        # convert the data and labels to NumPy arrays\n",
        "        data = np.array(data, dtype=\"float32\")\n",
        "        labels = np.array(labels, dtype=\"int\")\n",
        "\n",
        "        # return a 2-tuple of the A-Z data and labels\n",
        "        return (data, labels)\n",
        "\n",
        "\n",
        "def load_zero_nine_dataset():\n",
        "    # load the MNIST dataset and stack the training data and testing\n",
        "    # data together (we'll create our own training and testing splits\n",
        "    # later in the project)\n",
        "    ((trainData, trainLabels), (testData, testLabels)) = mnist.load_data()\n",
        "    data = np.vstack([trainData, testData])\n",
        "    labels = np.hstack([trainLabels, testLabels])\n",
        "    # return a 2-tuple of the MNIST data and labels\n",
        "    return (data, labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzoCG6kQYmIa",
        "outputId": "294beb1d-45c3-4c20-832c-08266f2cb496"
      },
      "source": [
        "# load the A-Z and MNIST datasets, respectively\n",
        "print(\"[INFO] loading datasets...\")\n",
        "\n",
        "(azData, azLabels) = load_az_dataset('/content/A_Z Handwritten Data.csv')\n",
        "(digitsData, digitsLabels) = load_zero_nine_dataset()\n",
        "\n",
        "# the MNIST dataset occupies the labels 0-9, so let's add 10 to every\n",
        "# A-Z label to ensure the A-Z characters are not incorrectly labeled\n",
        "# as digits\n",
        "azLabels += 10\n",
        "\n",
        "\n",
        "# stack the A-Z data and labels with the MNIST digits data and labels\n",
        "data = np.vstack([azData, digitsData])\n",
        "labels = np.hstack([azLabels, digitsLabels])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] loading datasets...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5I33FChXk9K"
      },
      "source": [
        "## Data Pre Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql-i71xqYzRu"
      },
      "source": [
        "# each image in the A-Z and MNIST digts datasets are 28x28 pixels;\n",
        "# however, the architecture we're using is designed for 32x32 images,\n",
        "# so we need to resize them to 32x32\n",
        "data = [cv2.resize(image, (32, 32)) for image in data]\n",
        "data = np.array(data, dtype=\"float32\")\n",
        "\n",
        "# add a channel dimension to every image in the dataset and scale the\n",
        "# pixel intensities of the images from [0, 255] down to [0, 1]\n",
        "data = np.expand_dims(data, axis=-1)\n",
        "data /= 255.0\n",
        "\n",
        "# convert the labels from integers to vectors\n",
        "le = LabelBinarizer()\n",
        "\n",
        "labels = le.fit_transform(labels)\n",
        "ounts = labels.sum(axis=0)\n",
        "\n",
        "# account for skew in the labeled data\n",
        "classTotals = labels.sum(axis=0)\n",
        "classWeight = {}\n",
        "\n",
        "# loop over all classes and calculate the class weight\n",
        "for i in range(0, len(classTotals)):\n",
        "    classWeight[i] = classTotals.max() / classTotals[i]\n",
        "\n",
        "# partition the data into training and testing splits using 80% of\n",
        "# the data for training and the remaining 20% for testing\n",
        "(trainX, testX, trainY, testY) = train_test_split(data,\n",
        "                                                  labels, test_size=0.20, stratify=None, random_state=42)\n",
        "\n",
        "# construct the image generator for data augmentation\n",
        "aug = ImageDataGenerator(rotation_range=10, zoom_range=0.05, width_shift_range=0.1,\n",
        "                         height_shift_range=0.1, shear_range=0.15, horizontal_flip=False, fill_mode=\"nearest\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkCUZh0GZgtq"
      },
      "source": [
        "## Model Compile and Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FJ3d0FfZiDp"
      },
      "source": [
        "class ResNet:\n",
        "    @staticmethod\n",
        "    def residual_module(data, K, stride, chanDim, red=False,\n",
        "                        reg=0.0001, bnEps=2e-5, bnMom=0.9):\n",
        "        # the shortcut branch of the ResNet module should be\n",
        "        # initialize as the input (identity) data\n",
        "        shortcut = data\n",
        "\n",
        "        # the first block of the ResNet module are the 1x1 CONVs\n",
        "        bn1 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
        "                                 momentum=bnMom)(data)\n",
        "        act1 = Activation(\"relu\")(bn1)\n",
        "        conv1 = Conv2D(int(K * 0.25), (1, 1), use_bias=False,\n",
        "                       kernel_regularizer=l2(reg))(act1)\n",
        "\n",
        "        # the second block of the ResNet module are the 3x3 CONVs\n",
        "        bn2 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
        "                                 momentum=bnMom)(conv1)\n",
        "        act2 = Activation(\"relu\")(bn2)\n",
        "        conv2 = Conv2D(int(K * 0.25), (3, 3), strides=stride,\n",
        "                       padding=\"same\", use_bias=False,\n",
        "                       kernel_regularizer=l2(reg))(act2)\n",
        "\n",
        "        # the third block of the ResNet module is another set of 1x1\n",
        "        # CONVs\n",
        "        bn3 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
        "                                 momentum=bnMom)(conv2)\n",
        "        act3 = Activation(\"relu\")(bn3)\n",
        "        conv3 = Conv2D(K, (1, 1), use_bias=False,\n",
        "                       kernel_regularizer=l2(reg))(act3)\n",
        "\n",
        "        # if we are to reduce the spatial size, apply a CONV layer to\n",
        "        # the shortcut\n",
        "        if red:\n",
        "            shortcut = Conv2D(K, (1, 1), strides=stride,\n",
        "                              use_bias=False, kernel_regularizer=l2(reg))(act1)\n",
        "\n",
        "        # add together the shortcut and the final CONV\n",
        "        x = add([conv3, shortcut])\n",
        "\n",
        "        # return the addition as the output of the ResNet module\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def build(width, height, depth, classes, stages, filters,\n",
        "              reg=0.0001, bnEps=2e-5, bnMom=0.9, dataset=\"cifar\"):\n",
        "        # initialize the input shape to be \"channels last\" and the\n",
        "        # channels dimension itself\n",
        "        inputShape = (height, width, depth)\n",
        "        chanDim = -1\n",
        "\n",
        "        # if we are using \"channels first\", update the input shape\n",
        "        # and channels dimension\n",
        "        if K.image_data_format() == \"channels_first\":\n",
        "            inputShape = (depth, height, width)\n",
        "            chanDim = 1\n",
        "\n",
        "        # set the input and apply BN\n",
        "        inputs = Input(shape=inputShape)\n",
        "        x = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
        "                               momentum=bnMom)(inputs)\n",
        "\n",
        "        # check if we are utilizing the CIFAR dataset\n",
        "        if dataset == \"cifar\":\n",
        "            # apply a single CONV layer\n",
        "            x = Conv2D(filters[0], (3, 3), use_bias=False,\n",
        "                       padding=\"same\", kernel_regularizer=l2(reg))(x)\n",
        "\n",
        "        # check to see if we are using the Tiny ImageNet dataset\n",
        "        elif dataset == \"tiny_imagenet\":\n",
        "            # apply CONV => BN => ACT => POOL to reduce spatial size\n",
        "            x = Conv2D(filters[0], (5, 5), use_bias=False,\n",
        "                       padding=\"same\", kernel_regularizer=l2(reg))(x)\n",
        "            x = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
        "                                   momentum=bnMom)(x)\n",
        "            x = Activation(\"relu\")(x)\n",
        "            x = ZeroPadding2D((1, 1))(x)\n",
        "            x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
        "\n",
        "        # loop over the number of stages\n",
        "        for i in range(0, len(stages)):\n",
        "            # initialize the stride, then apply a residual module\n",
        "            # used to reduce the spatial size of the input volume\n",
        "            stride = (1, 1) if i == 0 else (2, 2)\n",
        "            x = ResNet.residual_module(x, filters[i + 1], stride,\n",
        "                                       chanDim, red=True, bnEps=bnEps, bnMom=bnMom)\n",
        "\n",
        "            # loop over the number of layers in the stage\n",
        "            for j in range(0, stages[i] - 1):\n",
        "                # apply a ResNet module\n",
        "                x = ResNet.residual_module(x, filters[i + 1],\n",
        "                                           (1, 1), chanDim, bnEps=bnEps, bnMom=bnMom)\n",
        "\n",
        "        # apply BN => ACT => POOL\n",
        "        x = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
        "                               momentum=bnMom)(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        x = AveragePooling2D((8, 8))(x)\n",
        "\n",
        "        # softmax classifier\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(classes, kernel_regularizer=l2(reg))(x)\n",
        "        x = Activation(\"softmax\")(x)\n",
        "\n",
        "        # create the model\n",
        "        model = Model(inputs, x, name=\"resnet\")\n",
        "\n",
        "        # return the constructed network architecture\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "1yYeaS9EZ3kZ",
        "outputId": "4790a9fd-37c2-480a-b719-7ed6bc92d4a1"
      },
      "source": [
        "# initialize the number of epochs to train for, initial learning rate,\n",
        "# and batch size\n",
        "EPOCHS = 50\n",
        "INIT_LR = 1e-1\n",
        "BS = 128\n",
        "\n",
        "# initialize and compile our deep neural network\n",
        "print(\"[INFO] compiling model...\")\n",
        "\n",
        "opt = SGD(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
        "model = ResNet.build(32, 32, 1, len(le.classes_), (3, 3, 3),\n",
        "                     (64, 64, 128, 256), reg=0.0005)\n",
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=opt, metrics=[\"accuracy\"])\n",
        "\n",
        "# train the network\n",
        "print(\"[INFO] training network...\")\n",
        "\n",
        "H = model.fit(\n",
        "    aug.flow(trainX, trainY, batch_size=BS), validation_data=(testX, testY), steps_per_epoch=len(trainX) // BS, epochs=EPOCHS,\n",
        "    class_weight=classWeight,\n",
        "    verbose=1)\n",
        "\n",
        "# define the list of label names\n",
        "labelNames = \"0123456789\"\n",
        "labelNames += \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "labelNames = [l for l in labelNames]\n",
        "\n",
        "# evaluate the network\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predictions = model.predict(testX, batch_size=BS)\n",
        "print(classification_report(testY.argmax(axis=1),\n",
        "                            predictions.argmax(axis=1), target_names=labelNames))\n",
        "\n",
        "# save the model to disk\n",
        "print()\n",
        "model.save('ocr_model.h5', save_format=\"h5\")\n",
        "\n",
        "# construct a plot that plots and saves the training history\n",
        "N = np.arange(0, EPOCHS)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.title(\"Trainning Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig(args[\"plot\"])\n",
        "\n",
        "images = [] \n",
        "# randomly select a few testing characters\n",
        "for i in np.random.choice(np.arange(0, len(testY)), size=(49,)):\n",
        "    # classify the character\n",
        "    probs = model.predict(testX[np.newaxis, i])\n",
        "prediction = probs.argmax(axis=1)\n",
        "label = labelNames[prediction[0]]\n",
        "\n",
        "# extract the image from the test data and initialize the text\n",
        "# label color as green (correct)\n",
        "image = (testX[i] * 255).astype(\"uint8\")\n",
        "color = (0, 255, 0)\n",
        "\n",
        "# otherwise, the class label prediction is incorrect\n",
        "if prediction[0] != np.argmax(testY[i]):\n",
        "    color = (0, 0, 255)\n",
        "\n",
        "# merge the channels into one image, resize the image from 32x32\n",
        "# to 96x96 so we can better see it and then draw the predicted\n",
        "# label on the image\n",
        "image = cv2.merge([image] * 3)\n",
        "image = cv2.resize(image, (96, 96), interpolation=cv2.INTER_LINEAR)\n",
        "cv2.putText(image, label, (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.75,\n",
        "color, 2)\n",
        "\n",
        "# add the image to our list of output images\n",
        "images.append(image)\n",
        "\n",
        "# construct the montage for the images\n",
        "montage = build_montages(images, (96, 96), (7, 7))[0]\n",
        "\n",
        "# show the output montage\n",
        "print(\"OCR Results\")\n",
        "cv2_imshow(montage)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] compiling model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] training network...\n",
            "Epoch 1/50\n",
            "186/437 [===========>..................] - ETA: 11:01 - loss: 1.4612 - accuracy: 0.6965"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-699df2f13ef0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0maug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassWeight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     verbose=1)\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# define the list of label names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en4FEtwTcIrp"
      },
      "source": [
        "## Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsn4L47-cJ1q"
      },
      "source": [
        "# load the handwriting OCR model\n",
        "print(\"[INFO] loading handwriting OCR model...\")\n",
        "model = load_model('ocr_model.h5')\n",
        "\n",
        "# load the input image from disk, convert it to grayscale, and blur\n",
        "# it to reduce noise\n",
        "image = cv2.imread('plate.png')\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "# perform edge detection, find contours in the edge map, and sort the\n",
        "# resulting contours from left-to-right\n",
        "edged = cv2.Canny(blurred, 30, 150)\n",
        "cnts = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL,\n",
        "\tcv2.CHAIN_APPROX_SIMPLE)\n",
        "cnts = imutils.grab_contours(cnts)\n",
        "cnts = sort_contours(cnts, method=\"left-to-right\")[0]\n",
        "\n",
        "# initialize the list of contour bounding boxes and associated\n",
        "# characters that we'll be OCR'ing\n",
        "chars = []\n",
        "\n",
        "# loop over the contours\n",
        "for c in cnts:\n",
        "\t# compute the bounding box of the contour\n",
        "\t(x, y, w, h) = cv2.boundingRect(c)\n",
        "\n",
        "\t# filter out bounding boxes, ensuring they are neither too small\n",
        "\t# nor too large\n",
        "\tif (w >= 5 and w <= 150) and (h >= 15 and h <= 120):\n",
        "\t\t# extract the character and threshold it to make the character\n",
        "\t\t# appear as *white* (foreground) on a *black* background, then\n",
        "\t\t# grab the width and height of the thresholded image\n",
        "\t\troi = gray[y:y + h, x:x + w]\n",
        "\t\tthresh = cv2.threshold(roi, 0, 255,\n",
        "\t\t\tcv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
        "\t\t(tH, tW) = thresh.shape\n",
        "\n",
        "\t\t# if the width is greater than the height, resize along the\n",
        "\t\t# width dimension\n",
        "\t\tif tW > tH:\n",
        "\t\t\tthresh = imutils.resize(thresh, width=32)\n",
        "\n",
        "\t\t# otherwise, resize along the height\n",
        "\t\telse:\n",
        "\t\t\tthresh = imutils.resize(thresh, height=32)\n",
        "\n",
        "\t\t# re-grab the image dimensions (now that its been resized)\n",
        "\t\t# and then determine how much we need to pad the width and\n",
        "\t\t# height such that our image will be 32x32\n",
        "\t\t(tH, tW) = thresh.shape\n",
        "\t\tdX = int(max(0, 32 - tW) / 2.0)\n",
        "\t\tdY = int(max(0, 32 - tH) / 2.0)\n",
        "\n",
        "\t\t# pad the image and force 32x32 dimensions\n",
        "\t\tpadded = cv2.copyMakeBorder(thresh, top=dY, bottom=dY,\n",
        "\t\t\tleft=dX, right=dX, borderType=cv2.BORDER_CONSTANT,\n",
        "\t\t\tvalue=(0, 0, 0))\n",
        "\t\tpadded = cv2.resize(padded, (32, 32))\n",
        "\n",
        "\t\t# prepare the padded image for classification via our\n",
        "\t\t# handwriting OCR model\n",
        "\t\tpadded = padded.astype(\"float32\") / 255.0\n",
        "\t\tpadded = np.expand_dims(padded, axis=-1)\n",
        "\n",
        "\t\t# update our list of characters that will be OCR'd\n",
        "\t\tchars.append((padded, (x, y, w, h)))\n",
        "\n",
        "# extract the bounding box locations and padded characters\n",
        "boxes = [b[1] for b in chars]\n",
        "chars = np.array([c[0] for c in chars], dtype=\"float32\")\n",
        "\n",
        "# OCR the characters using our handwriting recognition model\n",
        "preds = model.predict(chars)\n",
        "\n",
        "# define the list of label names\n",
        "labelNames = \"0123456789\"\n",
        "labelNames += \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "labelNames = [l for l in labelNames]\n",
        "\n",
        "# loop over the predictions and bounding box locations together\n",
        "for (pred, (x, y, w, h)) in zip(preds, boxes):\n",
        "\t# find the index of the label with the largest corresponding\n",
        "\t# probability, then extract the probability and label\n",
        "\ti = np.argmax(pred)\n",
        "\tprob = pred[i]\n",
        "\tlabel = labelNames[i]\n",
        "\n",
        "\t# draw the prediction on the image\n",
        "\tprint(\"[INFO] {} - {:.2f}%\".format(label, prob * 100))\n",
        "\tcv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\tcv2.putText(image, label, (x - 10, y - 10),\n",
        "\t\tcv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
        "\n",
        "# show the image\n",
        "print(\"Image\")\n",
        "cv2_imshow(image)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "GZqevPkphvUF",
        "outputId": "72850bb8-bc84-45ca-969a-d5a90ae16c13"
      },
      "source": [
        "# load the handwriting OCR model\n",
        "print(\"[INFO] loading handwriting OCR model...\")\n",
        "model = load_model('ocr_model.h5')\n",
        "\n",
        "# load the input image from disk, convert it to grayscale, and blur\n",
        "# it to reduce noise\n",
        "image = cv2.imread('plate.png')\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "cnts = cv2.findContours(gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "cnts = imutils.grab_contours(cnts)\n",
        "cnts = sort_contours(cnts, method=\"left-to-right\")[0]\n",
        "\n",
        "# initialize the list of contour bounding boxes and associated\n",
        "# characters that we'll be OCR'ing\n",
        "chars = []\n",
        "\n",
        "ima = image.copy()\n",
        "for c in cnts:\n",
        "  cv2.rectangle(ima, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "cv2_imshow(ima)\n",
        "\n",
        "# loop over the contours\n",
        "for c in cnts:\n",
        "  (x, y, w, h) = cv2.boundingRect(c)\n",
        "  roi = gray[y:y + h, x:x + w]\n",
        "  thresh = cv2.threshold(roi, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
        "  (tH, tW) = thresh.shape\n",
        "\n",
        "\n",
        "  (tH, tW) = thresh.shape\n",
        "  dX = int(max(0, 32 - tW) / 2.0)\n",
        "  dY = int(max(0, 32 - tH) / 2.0)\n",
        "\n",
        "  # pad the image and force 32x32 dimensions\n",
        "  padded = cv2.copyMakeBorder(thresh, top=dY, bottom=dY,\n",
        "    left=dX, right=dX, borderType=cv2.BORDER_CONSTANT,\n",
        "    value=(0, 0, 0))\n",
        "  padded = cv2.resize(padded, (32, 32))\n",
        "\n",
        "  # prepare the padded image for classification via our\n",
        "  # handwriting OCR model\n",
        "  padded = padded.astype(\"float32\") / 255.0\n",
        "  padded = np.expand_dims(padded, axis=-1)\n",
        "\n",
        "  # update our list of characters that will be OCR'd\n",
        "  chars.append((padded, (x, y, w, h)))\n",
        "\n",
        "# extract the bounding box locations and padded characters\n",
        "boxes = [b[1] for b in chars]\n",
        "chars = np.array([c[0] for c in chars], dtype=\"float32\")\n",
        "\n",
        "# OCR the characters using our handwriting recognition model\n",
        "preds = model.predict(chars)\n",
        "\n",
        "# define the list of label names\n",
        "labelNames = \"0123456789\"\n",
        "labelNames += \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "labelNames = [l for l in labelNames]\n",
        "\n",
        "# loop over the predictions and bounding box locations together\n",
        "for (pred, (x, y, w, h)) in zip(preds, boxes):\n",
        "\t# find the index of the label with the largest corresponding\n",
        "\t# probability, then extract the probability and label\n",
        "\ti = np.argmax(pred)\n",
        "\tprob = pred[i]\n",
        "\tlabel = labelNames[i]\n",
        "\n",
        "\t# draw the prediction on the image\n",
        "\tprint(\"[INFO] {} - {:.2f}%\".format(label, prob * 100))\n",
        "\tcv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\tcv2.putText(image, label, (x - 10, y - 10),\n",
        "\t\tcv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
        "\n",
        "# show the image\n",
        "print(\"Image\")\n",
        "cv2_imshow(image)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] loading handwriting OCR model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=212x40 at 0x7FEF654A1B10>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fefd85be830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[INFO] 6 - 61.42%\n",
            "Image\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=212x40 at 0x7FEFD8605C50>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUlTwF05ifs-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}